{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 699: Tarea calificada 2, INAR 23-24\n",
    "\n",
    "## Generación de texto seq2seq model\n",
    "## A partir de textos de parlamentarios españoles (anteriores a 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nota importante\n",
    "\n",
    "Esta tarea en su versión 2023-24 surge del excelente trabajo de varios compañeros del curso 2022-23, que aunque yo proporcioné un dataset de textos a partir de las intervenciones de parlamentarios (los líderes de varios partidos en 2021-22, alguno de los cuales ya no está en la política española), hicieron un extraordinario \"escrapeo\" de la web del Congreso de los Diputados y enriquecieron de forma notable el dataset. Este es el que propongo para esta tarea.\n",
    "\n",
    "Debo decir que si hay un texto (o lenguaje natural) libre de derechos y especialmente actual, son las intervenciones (estrictamente **públicas**) de los representantes elegidos en elecciones, y que el Congreso debería facilitar, no ya para su uso en estas tareas, sino para cualquier estudioso del español, o de la política, o de la psicología de los políticos.\n",
    "\n",
    "Por supuesto, esto son opiniones estrictamete mías, en el momento concreto en que las escribo, y sencillamente quiero hacer homenaje a los que colaboraron tanto con este trabajo que espero encontréis interesante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿De qué trata esta tarea?\n",
    "\n",
    "Pues ni más ni menos que de generar texto en español a partir de texto de parlamentarios, basado en el tutorial que hemos seguido en clase:\n",
    "\n",
    "https://www.tensorflow.org/text/tutorials/text_generation?hl=es-419\n",
    "\n",
    "Para facilitar la tarea se propone un pre-proceso (basado en la tarea 2021-22), y la tarea se concreta en el modelo para generar texto y en las pruebas de la calidad del texto generado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calificación\n",
    "\n",
    "Está explicada en la entrada correspondiente de Blackboard. Básicamente, hay un mínimo que consiste en proponer tres modelos de red recurrente, uno para cada parlamentario, entrenarlos, y **evaluarlos** generando texto y comentando su calidad.\n",
    "\n",
    "Para llegar a la máxima nota, propongo poner a dialogar los tres modelos.\n",
    "\n",
    "Pero por supuesto, valoraré el trabajo de construcción del modelo. Para esta tarea no hay una \"medida\" como la accuracy en la tarea 1. Será relativamente subjetiva. Por eso parece aconsejable comenzar con modelos pequeños o con pocas etapas e ir refinando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Para facilitar la tarea propongo unas cuantas casillas para cargar en memoria los textos, tres .txt que están incluidos en un .zip.\n",
    "\n",
    "## Nota importante\n",
    "\n",
    "La codificación (juego de caracteres) es UTF-8 y creo que debe seguir siendo así. *NO* abráis los .txt con el Notepad de Windows, sino con el Notepad+++ que os permitiría cambiarlo o devolverlo a UTF-8 (o Unicode si queréis).\n",
    "\n",
    "A pesar que la salida por pantalla (en mi sistema, un Linux) de caracteres ñ y acentuados parece que está mal, luego la generación de texto (insisto, lo he comprobado en mi sistema) es correcta en español.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:08.261025Z",
     "iopub.status.busy": "2022-05-03T11:14:08.260828Z",
     "iopub.status.idle": "2022-05-03T11:14:10.284556Z",
     "shell.execute_reply": "2022-05-03T11:14:10.283846Z"
    },
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "## Lectura de ficheros de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:10.288588Z",
     "iopub.status.busy": "2022-05-03T11:14:10.288339Z",
     "iopub.status.idle": "2022-05-03T11:14:10.512538Z",
     "shell.execute_reply": "2022-05-03T11:14:10.511842Z"
    },
    "id": "pD_55cOxLkAb"
   },
   "outputs": [],
   "source": [
    "datos_abascal   = \"intervencionesAbascal.txt\"\n",
    "datos_sanchez   = \"intervencionesSanchez.txt\"\n",
    "datos_casado    = \"intervencionesCasado.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### Leer los ficheros de datos\n",
    "\n",
    "Primero, abrimos el texto de Santiago Abascal, que es el más corto, y lo leemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:10.515842Z",
     "iopub.status.busy": "2022-05-03T11:14:10.515602Z",
     "iopub.status.idle": "2022-05-03T11:14:10.521336Z",
     "shell.execute_reply": "2022-05-03T11:14:10.520758Z"
    },
    "id": "aavnuByVymwK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto de Santiago Abascal: 22573 carácteres\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(datos_abascal, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# length of text is the number of characters in it\n",
    "print(f'Texto de Santiago Abascal: {len(text)} carácteres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:10.524482Z",
     "iopub.status.busy": "2022-05-03T11:14:10.523966Z",
     "iopub.status.idle": "2022-05-03T11:14:10.527481Z",
     "shell.execute_reply": "2022-05-03T11:14:10.526931Z"
    },
    "id": "Duhg9NrUymwO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Señor Sánchez, ¿cómo se atreve usted a hablarme de monólogos si siempre trae las respuestas escritas, si usted nunca contesta a mis preguntas? Conteste por lo menos hoy. ¿Qué va a hacer usted para impedir que VOX siga cruzando las líneas que dice ust\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:10.530172Z",
     "iopub.status.busy": "2022-05-03T11:14:10.529967Z",
     "iopub.status.idle": "2022-05-03T11:14:10.544918Z",
     "shell.execute_reply": "2022-05-03T11:14:10.544310Z"
    },
    "id": "IlCgQBRVymwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cual de los tres textos tiene el mayor vocabulario, para usar el mismo en los tres modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22573 carácteres, 81 únicos en intervencionesAbascal.txt\n",
      "239623 carácteres, 104 únicos en intervencionesSanchez.txt\n",
      "105940 carácteres, 92 únicos en intervencionesCasado.txt\n",
      "108 únicos en los tres textos\n"
     ]
    }
   ],
   "source": [
    "vocab_mayor = vocab\n",
    "\n",
    "textos = []\n",
    "\n",
    "for texto in [datos_abascal, datos_sanchez, datos_casado]:\n",
    "    text = open(texto, 'rb').read().decode(encoding='utf-8')\n",
    "    vocab = sorted(set(text))\n",
    "    print(f'{len(text)} carácteres, {len(vocab)} únicos en {texto}')\n",
    "    \n",
    "    if len(vocab) > len(vocab_mayor):\n",
    "        vocab_mayor = vocab\n",
    "    \n",
    "    textos.append(text)\n",
    "        \n",
    "vocab = sorted(set(textos[0] + textos[1] + textos[2]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'{vocab_size} únicos en los tres textos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## Procesar el texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### Vamos a vectorizar el texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como las redes neuronales no entienden carácteres sino números, vamos a vectorizar el texto. Para ello, vamos a crear dos *\"tablas de traducción\"*, uno para pasar de carácter a número y otro para pasar de número a carácter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:10.547650Z",
     "iopub.status.busy": "2022-05-03T11:14:10.547458Z",
     "iopub.status.idle": "2022-05-03T11:14:12.216225Z",
     "shell.execute_reply": "2022-05-03T11:14:12.215486Z"
    },
    "id": "a86OoYtO01go"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\x07': 0\n",
      "'\\n': 1\n",
      "' ': 2\n",
      "'!': 3\n",
      "'%': 4\n",
      "'&': 5\n",
      "'(': 6\n",
      "')': 7\n",
      "',': 8\n",
      "'-': 9\n",
      "'.': 10\n",
      "'0': 11\n",
      "'1': 12\n",
      "'2': 13\n",
      "'3': 14\n",
      "'4': 15\n",
      "'5': 16\n",
      "'6': 17\n",
      "'7': 18\n",
      "'8': 19\n"
     ]
    }
   ],
   "source": [
    "# Creamos un diccionario para asignar cada caracter a un entero\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "# Luego hacemos una lista con los carácteres ordenados por su entero\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# Vamos a ver que pinta tiene nuestro diccionario\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print(f'{repr(char)}: {char2idx[char]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya podemos vectorizar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Señor Sánchez' ---- carácteres mapeados a int ----> [42 56 93 66 69  2 42 90 65 54 59 56 77]\n"
     ]
    }
   ],
   "source": [
    "# Ahora podemos convertir todo el texto a enteros\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# Vamos a ver como queda el texto en enteros\n",
    "print(f'{repr(text[:13])} ---- carácteres mapeados a int ----> {text_as_int[:13]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "# Fases propuestas para la elaboración del modelo\n",
    "\n",
    "### 1. Crear los training examples y los targets\n",
    "\n",
    "Ahora vamos a divir nuestro texto en secuencia de carácteres. Cada secuencia tendrá `seq_length` carácteres de nuestro texto.\n",
    "Para cada secuencia de entrada, los targets correspondientes contienen la misma longitud de texto, excepto desplazada un carácter a la derecha.\n",
    "Por eso dividimos el texto en secuencias de `seq_length+1`. Por ejemplo, digamos que `seq_length` es 4 y nuestro texto es \"Hola\". La secuencia de entrada sería \"Hol\" y la secuencia de salida \"ola\".\n",
    "\n",
    "Para hacer esto, primero usamos la función `tf.data.Dataset.from_tensor_slices` para convertir el vector de texto en una secuencia de índices de caracteres.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuencias de 100 carácteres:\n",
      "'Señor Sánchez, sus recetas económicas son tan creíbles como sus promesas electorales, y encima propon'\n",
      "'en las mismas recetas fracasadas que nos llevaron a la peor crisis económica de nuestra historia: más'\n",
      "' despilfarro, más déficit y más impuestos. Pero el Partido Popular es un partido de Estado y también '\n",
      "'de Gobierno, aunque estemos temporalmente en la oposición. Por eso el lunes le ofrecí pactar los Pres'\n",
      "'upuestos Generales si rompe con los independentistas, una oferta, por cierto, a la que usted no ha co'\n"
     ]
    }
   ],
   "source": [
    "# Creamos un dataset de tensorflow con los enteros\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# Ahora vamos a dividir el texto en secuencias de 100 carácteres\n",
    "SEQ_LENGTH = 100\n",
    "sequences = char_dataset.batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
    "\n",
    "# Vamos a ver como son estas secuencias\n",
    "print(\"Secuencias de 100 carácteres:\")\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos de entrenamiento contiene tanto los datos de entrada (desde la posición 0 a la 99) como los de salida (desde la posición 1 a la 100). Por lo que necesitamos mapear el input y el target para crear el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  'Señor Sánchez, sus recetas económicas son tan creíbles como sus promesas electorales, y encima propo'\n",
      "Target:  'eñor Sánchez, sus recetas económicas son tan creíbles como sus promesas electorales, y encima propon'\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "# Ahora vamos a aplicar la función anterior a todas las secuencias\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# Vamos a ver como son las secuencias de entrada y salida\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target: ', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### 2. Crear los training batches\n",
    "\n",
    "Ahora ya podemos mezclar los datos y empaquetarlos en batches de 64 secuencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## 3. Crear el modelo\n",
    "\n",
    "Puedes usar cualquiera de los modelos (RNN, LSTM, GRU) que hemos visto en clase. Por supuesto, del tamaño del modelo (capas, neuronas en cada capa) así como de las épocas (más adelante) dependerá el tiempo de proceso en el .fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo de Abascal vamos a usar una RNN que contenga solo una capa LSTM. En concreto, definiremos una red neuronal de solo 3 capas:\n",
    "\n",
    "- Capa de entrada: una capa de tipo Embedding, que convierte los índices de los caracteres en vectores embedding de tamaño embedding_dim. En las opciones de la capa especificaremos el tamaño de nuestro vocabulario `(vocab_size)` y el tamaño de los vectores embedding `(embedding_dim)`. También indicaremos el tamaño del batch que vamos a usar `(batch_size)`.\n",
    "\n",
    "- Capa LSTM: una capa LSTM con `units=2048`, que es el número de neuronas recurrentes de la capa. También indicaremos con return_sequences=True que queremos predecir el carácter siguiente a todos los carácteres de entrada y no solo al último carácter. El argumento `stateful=True` explica el uso de las capacidades de memoria de la red entre batches: Si está en False, por cada nuevo batch se inicializan las memory cells (la parte de la red neuronal que preserva el estado de la red a través del tiempo), pero si está en True, por cada nuevo batch se mantienen las memory cells con las actualizaciones hechas durante la ejecución del batch anterior. El último argumento, `recurrent_initializer='glorot_uniform'`, es un que indica como se inicializan los pesos de las matrices internas de la capa LSTM. En estos casosm la distribución más común es la `glorot_uniform`.\n",
    "\n",
    "- Capa de salida: una capa Dense con `vocab_size` neuronas. Esta capa nos dará como salida un vector de tamaño `vocab_size` con las probabilidades de que el siguiente carácter sea cada uno de los carácteres del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (64, None, 512)           55296     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (64, None, 2048)          20979712  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (64, None, 108)           221292    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21256300 (81.09 MB)\n",
      "Trainable params: 21256300 (81.09 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim,\n",
    "                  batch_input_shape=[batch_size, None]),\n",
    "        LSTM(rnn_units, return_sequences=True,\n",
    "             recurrent_initializer='glorot_uniform',\n",
    "             stateful=True),\n",
    "        Dense(vocab_size)\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "embedding_dim = 512\n",
    "rnn_units = 2048\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos nuestra función de pérdida y el optimizador que vamos a usar para entrenar el modelo. En este caso, usaremos la función de pérdida `sparse_categorical_crossentropy` y el optimizador `Adam` con sus argumentos por defecto. Con esto ya podemos compilar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar la técnica de los checkpoints para no perder el progreso del entrenamiento si tenemos un fallo en el sistema. El único problema es que los checkpoints pueden llegar a ocupar mucho espacio muy rápidamente, por lo que es recomendable borrarlos después de entrenarlos y en su lugar guardar el modelo ya terminado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8gPwEjRzf-Z"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "## 4. Summary y fit del modelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 3s 144ms/step - loss: 2.4234\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 2.3454\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 2.2828\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 2.2257\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 2.1726\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 2.1147\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 2.0630\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 2.0071\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 1.9552\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 1.9094\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 1.8618\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 1.8149\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 1.7666\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 1.7241\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 1.6816\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 1.6430\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 1.5935\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 1.5537\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 1.5134\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 3s 187ms/step - loss: 1.4781\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 5s 321ms/step - loss: 1.4394\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 1.3935\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 1.3534\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 1.3120\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 1.2759\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 1.2395\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 1.2006\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 1.1599\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 1.1201\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 1.0814\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 1.0380\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 0.9950\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.9519\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.9067\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.8589\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.8074\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.7545\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.7082\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.6612\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 0.6108\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.5649\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 0.5150\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.4703\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 0.4370\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 3s 208ms/step - loss: 0.4023\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 0.3700\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.3411\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.3170\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.2970\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.2783\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.2642\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.2518\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.2430\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.2352\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 0.2249\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.2164\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.2072\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.2055\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1992\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1925\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1884\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1853\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1816\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1775\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1742\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.1699\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.1690\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.1671\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.1644\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1593\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.1597\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 0.1556\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.1526\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.1507\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.1524\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.1482\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.1449\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1435\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.1437\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1426\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.1387\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.1382\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.1370\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.1352\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.1345\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.1315\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.1301\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.1293\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.1260\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.1292\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1250\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.1248\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.1240\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.1229\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1216\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.1205\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.1203\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1187\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 0.1172\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.1149\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.1165\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.1151\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.1163\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.1162\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.1117\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1122\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1141\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1110\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.1103\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1105\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.1094\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.1086\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.1077\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 0.1059\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.1066\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1063\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.1070\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.1051\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1058\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.1049\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1044\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1044\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 0.1046\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1044\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.1020\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 3s 207ms/step - loss: 0.1019\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.1017\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 0.1007\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.1005\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0988\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.1007\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0977\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.0988\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.0988\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0969\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0997\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.0954\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.0954\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0941\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0939\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.0949\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0959\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0964\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.0940\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.0945\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0944\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0970\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.0944\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.0927\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0930\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0916\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.0920\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.0905\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0918\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0904\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0892\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0900\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0925\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0888\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0904\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.0911\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.0896\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.0890\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0893\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0896\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0892\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.0879\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 0.0878\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 3s 143ms/step - loss: 0.0898\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 5s 298ms/step - loss: 0.0887\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0876\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0855\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0886\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.0878\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.0869\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.0862\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0863\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0868\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.0864\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0844\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.0852\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.0843\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.0866\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0855\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0846\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 0.0843\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0841\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0859\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.0823\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0799\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.0847\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0846\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 0.0823\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0821\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0840\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0835\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.0812\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.0826\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.0821\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.0820\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "\n",
    "# Guardamos el modelo y los pesos\n",
    "model.save_weights('./modelos/abascal_weights.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos terminado de entranar el modelo ya no necesitamos los chekpoints, por lo que podemos borrarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borramos los checkpoints para no ocupar espacio\n",
    "\n",
    "import os, shutil\n",
    "folder = './training_checkpoints/'\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## 5. Genera texto y evalúa su calidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para generar texto a partir del modelo, ahora necesitamos un `batch_size` de 1, por lo que tenemos que rehacer el modelo y cargar los pesos de nuestro modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights('./modelos/abascal_weights.keras')\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.save('./modelos/abascal.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear una función que con un texto de entrada nos genere texto. La variable `num_generate` indica cuantos carácteres se generarán y la variable `temperature` indica cuanto varía de los texto originales (con temperaturas altas el modelo será más creativo, pero a costa de cometer más errores). La temperatura está comprendida entre 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \n",
    "    num_generate = 500\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    \n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "    temperature = 0.5\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    # Bucle para generar los carácteres\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0) # Reducir la dimensión del batch ya que el modelo está entrenado con batch_size=64 y ahora estamos generando con batch_size=1\n",
    "        \n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        \n",
    "    return (''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " y los nacionalistas, y eso que ha hecho trampas con todos los instrumentos del Estado: con el artículo 155? Si no lo hace usted respetar, señor Sánchez. Ha elegido a los radicales, aquellos que van contra la tradición europea, la para proteger a los españoles y deja de insultar a la oposición? Haga algo, señor Sánchez, que para gestionar el mando único y la limitación de movimientos sin tener que recurrir a la legislación básica en vigor, como ya han hecho los países de nuestro entorno. Tal y c\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"La izquierda\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que enlaza bien la cadena de entrada para que tenga sentido y parece que más o menos tiene sentido lo que dice, pero no es muy coherente. También usa mal los signos de interrogación y algunos artículos. Aun así no es de extrañar, considerando que el dataset de Abascal es el más pequeño de los tres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4QwTjAM6A2O"
   },
   "source": [
    "## 6. Trabajo adicional\n",
    "\n",
    "Por ejemplo, poner en cadena los tres modelos para que \"dialoguen\" entre sí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Pedro Sánchez\n",
    "\n",
    "El modelo de Pedro Sánchez será similar al de Abascal, una RNN con una capa LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el dataset de entrenamiento de Pedro Sánchez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir el texto\n",
    "text = open(datos_sanchez, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# Vectorizar el texto\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# Secuencias de 100 carácteres\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "SEQ_LENGTH = 100\n",
    "sequences = char_dataset.batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
    "\n",
    "# Dataset de entrenamiento\n",
    "dataset = sequences.map(split_input_target)\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos el modelo de Pedro Sánchez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo de Pedro Sánchez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "37/37 [==============================] - 5s 113ms/step - loss: 3.3781\n",
      "Epoch 2/200\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 2.5602\n",
      "Epoch 3/200\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 2.2155\n",
      "Epoch 4/200\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 2.0314\n",
      "Epoch 5/200\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 1.8559\n",
      "Epoch 6/200\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 1.7195\n",
      "Epoch 7/200\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 1.5472\n",
      "Epoch 8/200\n",
      "37/37 [==============================] - 4s 109ms/step - loss: 1.4205\n",
      "Epoch 9/200\n",
      "37/37 [==============================] - 4s 109ms/step - loss: 1.3130\n",
      "Epoch 10/200\n",
      "37/37 [==============================] - 4s 109ms/step - loss: 1.2266\n",
      "Epoch 11/200\n",
      "37/37 [==============================] - 4s 109ms/step - loss: 1.1507\n",
      "Epoch 12/200\n",
      "37/37 [==============================] - 4s 109ms/step - loss: 1.0821\n",
      "Epoch 13/200\n",
      "37/37 [==============================] - 4s 109ms/step - loss: 1.0244\n",
      "Epoch 14/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.9690\n",
      "Epoch 15/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.9163\n",
      "Epoch 16/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.8633\n",
      "Epoch 17/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.8152\n",
      "Epoch 18/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.7593\n",
      "Epoch 19/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.7077\n",
      "Epoch 20/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.6562\n",
      "Epoch 21/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.6026\n",
      "Epoch 22/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.5521\n",
      "Epoch 23/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.5025\n",
      "Epoch 24/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.4572\n",
      "Epoch 25/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.4186\n",
      "Epoch 26/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.3815\n",
      "Epoch 27/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.3528\n",
      "Epoch 28/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.3250\n",
      "Epoch 29/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.3022\n",
      "Epoch 30/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.2853\n",
      "Epoch 31/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.2704\n",
      "Epoch 32/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.2582\n",
      "Epoch 33/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.2459\n",
      "Epoch 34/200\n",
      "37/37 [==============================] - 4s 112ms/step - loss: 0.2385\n",
      "Epoch 35/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.2272\n",
      "Epoch 36/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.2200\n",
      "Epoch 37/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.2141\n",
      "Epoch 38/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.2070\n",
      "Epoch 39/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.2031\n",
      "Epoch 40/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1974\n",
      "Epoch 41/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1948\n",
      "Epoch 42/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1908\n",
      "Epoch 43/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1858\n",
      "Epoch 44/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1815\n",
      "Epoch 45/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1784\n",
      "Epoch 46/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1753\n",
      "Epoch 47/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1726\n",
      "Epoch 48/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1700\n",
      "Epoch 49/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1672\n",
      "Epoch 50/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1649\n",
      "Epoch 51/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1617\n",
      "Epoch 52/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1613\n",
      "Epoch 53/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1591\n",
      "Epoch 54/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1573\n",
      "Epoch 55/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1567\n",
      "Epoch 56/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1531\n",
      "Epoch 57/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1527\n",
      "Epoch 58/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1511\n",
      "Epoch 59/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1487\n",
      "Epoch 60/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1475\n",
      "Epoch 61/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1462\n",
      "Epoch 62/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1469\n",
      "Epoch 63/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1464\n",
      "Epoch 64/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1428\n",
      "Epoch 65/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1418\n",
      "Epoch 66/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1421\n",
      "Epoch 67/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1402\n",
      "Epoch 68/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1397\n",
      "Epoch 69/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1387\n",
      "Epoch 70/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1383\n",
      "Epoch 71/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1359\n",
      "Epoch 72/200\n",
      "37/37 [==============================] - 4s 112ms/step - loss: 0.1364\n",
      "Epoch 73/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1363\n",
      "Epoch 74/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1337\n",
      "Epoch 75/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1331\n",
      "Epoch 76/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1325\n",
      "Epoch 77/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1334\n",
      "Epoch 78/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1308\n",
      "Epoch 79/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1311\n",
      "Epoch 80/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1295\n",
      "Epoch 81/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1289\n",
      "Epoch 82/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1280\n",
      "Epoch 83/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1274\n",
      "Epoch 84/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1284\n",
      "Epoch 85/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1291\n",
      "Epoch 86/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1264\n",
      "Epoch 87/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1266\n",
      "Epoch 88/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1269\n",
      "Epoch 89/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1249\n",
      "Epoch 90/200\n",
      "37/37 [==============================] - 4s 112ms/step - loss: 0.1269\n",
      "Epoch 91/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1240\n",
      "Epoch 92/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1244\n",
      "Epoch 93/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1251\n",
      "Epoch 94/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1227\n",
      "Epoch 95/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1226\n",
      "Epoch 96/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1220\n",
      "Epoch 97/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1222\n",
      "Epoch 98/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1219\n",
      "Epoch 99/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1203\n",
      "Epoch 100/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1210\n",
      "Epoch 101/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1211\n",
      "Epoch 102/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1205\n",
      "Epoch 103/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1201\n",
      "Epoch 104/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1204\n",
      "Epoch 105/200\n",
      "37/37 [==============================] - 4s 112ms/step - loss: 0.1192\n",
      "Epoch 106/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1193\n",
      "Epoch 107/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1179\n",
      "Epoch 108/200\n",
      "37/37 [==============================] - 4s 112ms/step - loss: 0.1187\n",
      "Epoch 109/200\n",
      "37/37 [==============================] - 4s 112ms/step - loss: 0.1163\n",
      "Epoch 110/200\n",
      "37/37 [==============================] - 4s 112ms/step - loss: 0.1180\n",
      "Epoch 111/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1162\n",
      "Epoch 112/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1170\n",
      "Epoch 113/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1164\n",
      "Epoch 114/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1154\n",
      "Epoch 115/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1153\n",
      "Epoch 116/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1157\n",
      "Epoch 117/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1146\n",
      "Epoch 118/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1155\n",
      "Epoch 119/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1166\n",
      "Epoch 120/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1161\n",
      "Epoch 121/200\n",
      "37/37 [==============================] - 4s 112ms/step - loss: 0.1165\n",
      "Epoch 122/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1164\n",
      "Epoch 123/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1145\n",
      "Epoch 124/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1142\n",
      "Epoch 125/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1139\n",
      "Epoch 126/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1124\n",
      "Epoch 127/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1124\n",
      "Epoch 128/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1124\n",
      "Epoch 129/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1116\n",
      "Epoch 130/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1125\n",
      "Epoch 131/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1119\n",
      "Epoch 132/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1132\n",
      "Epoch 133/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1107\n",
      "Epoch 134/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1127\n",
      "Epoch 135/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1116\n",
      "Epoch 136/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1088\n",
      "Epoch 137/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1085\n",
      "Epoch 138/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1094\n",
      "Epoch 139/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1086\n",
      "Epoch 140/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1094\n",
      "Epoch 141/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1110\n",
      "Epoch 142/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1120\n",
      "Epoch 143/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1096\n",
      "Epoch 144/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1108\n",
      "Epoch 145/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1137\n",
      "Epoch 146/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1136\n",
      "Epoch 147/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1151\n",
      "Epoch 148/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1147\n",
      "Epoch 149/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1125\n",
      "Epoch 150/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1146\n",
      "Epoch 151/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1147\n",
      "Epoch 152/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1166\n",
      "Epoch 153/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1165\n",
      "Epoch 154/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1173\n",
      "Epoch 155/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1139\n",
      "Epoch 156/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1148\n",
      "Epoch 157/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1122\n",
      "Epoch 158/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1080\n",
      "Epoch 159/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1062\n",
      "Epoch 160/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1053\n",
      "Epoch 161/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1042\n",
      "Epoch 162/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1023\n",
      "Epoch 163/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1014\n",
      "Epoch 164/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1012\n",
      "Epoch 165/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1015\n",
      "Epoch 166/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1003\n",
      "Epoch 167/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.0995\n",
      "Epoch 168/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.0989\n",
      "Epoch 169/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.0997\n",
      "Epoch 170/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.0993\n",
      "Epoch 171/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1003\n",
      "Epoch 172/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.0993\n",
      "Epoch 173/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.0999\n",
      "Epoch 174/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1001\n",
      "Epoch 175/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1004\n",
      "Epoch 176/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1019\n",
      "Epoch 177/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1023\n",
      "Epoch 178/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1039\n",
      "Epoch 179/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1044\n",
      "Epoch 180/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1097\n",
      "Epoch 181/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1204\n",
      "Epoch 182/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1485\n",
      "Epoch 183/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.2208\n",
      "Epoch 184/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.3057\n",
      "Epoch 185/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.3215\n",
      "Epoch 186/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.2721\n",
      "Epoch 187/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.2325\n",
      "Epoch 188/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1982\n",
      "Epoch 189/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1694\n",
      "Epoch 190/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1506\n",
      "Epoch 191/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1331\n",
      "Epoch 192/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1223\n",
      "Epoch 193/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1161\n",
      "Epoch 194/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1105\n",
      "Epoch 195/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1066\n",
      "Epoch 196/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1042\n",
      "Epoch 197/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1042\n",
      "Epoch 198/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.1031\n",
      "Epoch 199/200\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.1015\n",
      "Epoch 200/200\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 0.0999\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS)\n",
    "\n",
    "# Guardamos el modelo y los pesos\n",
    "model.save_weights('./modelos/sanchez_weights.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el modelo para generar texto de Pedro Sánchez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights('./modelos/sanchez_weights.keras')\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.save('./modelos/sanchez.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos el texto generado por Pedro Sánchez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Partido Popular resultar el conjunto de instituciones por aumentar el número de test, que se está elevando. España es uno de los principales valores de nuestra Carta Magna la igualdad, que ustedes atacan cuando el Gobierno de España defenden un objetivo marcado por la Unión Europea de que tiene que haber proyectos incluso transnacionales, sino la de hace ochenta años. La nuestra este Gobierno y, por tanto, no vamos a dejar a nadie quedel 14 de marzo, en Bruselas; del Consejo Europeo, celebrado \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Los impuestos del\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que ahora el texto generado tiene menos errores gramaticales y está mejor cohesionado, pero sigue sin tener mucho sentido. Vamos a intentar modificar la estructura del modelo para ver si conseguimos mejores resultados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuevo modelo de Pedro Sánchez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim,\n",
    "                  batch_input_shape=[batch_size, None]),\n",
    "        LSTM(rnn_units, return_sequences=True,\n",
    "             recurrent_initializer='glorot_uniform',\n",
    "             stateful=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(rnn_units, return_sequences=True,\n",
    "             recurrent_initializer='glorot_uniform',\n",
    "             stateful=True),\n",
    "        Dense(vocab_size)\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "embedding_dim = 512\n",
    "rnn_units = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Pablo Casado (GRU)\n",
    "\n",
    "Esta vez en vez de usar una LSTM como en los anteriores modelos, vamos a usar una RNN con capa GRU. La GRU es una versión simplificada de la LSTM, que tiene menos parámetros y por lo tanto es más rápida de entrenar. La GRU tiene dos puertas (gates) en vez de tres como la LSTM. \n",
    "\n",
    "Que tenga más parámetros le puede dar más capacidad de aprendizaje, pero también puede hacer que el modelo tarde más en entrenar y que sea más propenso al overfitting. Por lo tanto, es posible que la GRU nos venga bien, ya que nuestro dataset es pequeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el dataset de entrenamiento de Pablo Casado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir el texto\n",
    "text = open(datos_casado, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# Vectorizar el texto\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# Secuencias de 100 carácteres\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "SEQ_LENGTH = 100\n",
    "sequences = char_dataset.batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
    "\n",
    "# Dataset de entrenamiento\n",
    "dataset = sequences.map(split_input_target)\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el modelo de Pablo Casado, con la capa de GRU en vez de LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_gru(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size,embedding_dim,\n",
    "    batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,return_sequences=True,\n",
    "    stateful=True,recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)]\n",
    "  )\n",
    "    \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_gru(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo de Pablo Casado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "16/16 [==============================] - 3s 96ms/step - loss: 5.0457\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 3.1420\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 2.7586\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 2.5321\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 2.4528\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 2.4048\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 2.3686\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 2.3322\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 2.2943\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 2.2577\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 2.2255\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 2.1938\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 2.1663\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 2.1400\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 2.1131\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 2.0862\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 2.0625\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 2.0364\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 2.0148\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.9904\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.9694\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.9493\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.9259\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.9055\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.8873\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.8670\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.8490\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.8326\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.8128\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.7966\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 1.7807\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.7662\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 1.7529\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 1.7429\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.7289\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.7144\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.6997\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.6926\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.6789\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.6668\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.6561\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.6427\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.6355\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.6265\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.6171\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.6087\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.5978\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.5912\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.5818\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.5759\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.5667\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.5598\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.5496\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.5479\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.5408\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.5313\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 1.5234\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.5164\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.5080\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.5069\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.5000\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.4939\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.4930\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.4806\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 1.4741\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.4742\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.4674\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.4602\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.4546\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.4520\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.4502\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.4421\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.4375\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.4278\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.4221\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.4253\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 1.4136\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.4095\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 1.4067\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.4028\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.3972\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3954\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3953\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3862\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.3822\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3834\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3774\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3713\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3672\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3659\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3616\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3555\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.3522\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3496\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.3408\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3417\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3401\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3364\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3284\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3263\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3244\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3246\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3211\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3169\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3097\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.3099\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3050\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.3010\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2992\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2981\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2936\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2889\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2882\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2855\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2855\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2764\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 1.2755\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2678\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2753\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2664\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.2668\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.2658\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.2637\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2611\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.2558\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.2523\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.2519\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.2461\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2495\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2479\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2415\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.2384\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.2357\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2288\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.2270\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2237\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.2257\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2229\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.2237\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2203\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.2191\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2160\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.2172\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2098\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2111\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2019\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.2040\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2051\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.2069\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1986\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1943\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1953\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1902\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1927\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1841\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1866\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1822\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1850\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1835\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1802\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1787\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1778\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1760\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1690\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1668\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1661\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1631\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1644\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1643\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1638\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1583\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1562\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1533\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1532\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1504\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1532\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1549\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1515\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1417\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1418\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1424\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1399\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1352\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1384\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1367\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1371\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1320\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1314\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1317\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1269\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1240\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1208\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1226\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1180\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1142\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1126\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1147\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1154\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1151\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.1122\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "history = model.fit(dataset, epochs=EPOCHS)\n",
    "\n",
    "# Guardamos el modelo y los pesos\n",
    "model.save_weights('./modelos/casado_weights.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a generar texto con el modelo de Casado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_gru(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights('./modelos/casado_weights.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teareceones, electorezados.\n",
      "Señores?\n",
      "Señora. Leyes, ya enañesas.\n",
      "Acabaremos, estemeza; estánes. ¿Parates.\n",
      "Acabata, estempetencia.\n",
      "Acabaremos?\n",
      "Acabandonadas.\n",
      "Señoras, aunquezarle. ¿Quezarecente, este, este, estemeación.\n",
      "Señores? ¿Qué esabereza, estemplaza; electagaderales?\n",
      "Señora? Señora?\n",
      "Señores. Señor, Autónomentenezarte:?\n",
      "¿Qué quedenea, estemezón. La Leye, ¿qué estemezó encepteraremoses, aquelezáles. Señores? ¿Que estemezarenes. Nosotros. Ya estabelezades. Leyes, ustedes. Ya electa: ¿persemos \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"La luz está muy cara\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que aunque entrena muy rápido, el modelo de Pablo Casado no consigue formar palabras coherentes. Al cambiar la capa de LSTM a GRU el modelo no funciona bien, es posible que sea porque la GRU tiene menos parámetros y por lo tanto no es capaz de aprender bien el dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversación entre Pedro Sánchez y Santiago Abascal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para similar una especie de conversación entre Pedro Sánchez y Santiago Abascal, vamos a darle como input a los modelos los outputs del otro modelo. Es decir, vamos a darle como input al modelo de Pedro Sánchez el texto generado por el modelo de Santiago Abascal, y viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanchez:  política. Sobre el plano del Gobierno, le diré que hemos tenido que gestionar más rápido, y espoco vamos avanzando, señor Casado, usted ya reconoce que no estamos en quiebra, que nos recuperamos. Ahí están las previsiones del Fondo Monetario Internacional: una caída bruselas, como he dicho antes, cuestionando la democracia. Y cuando la negociación entró en una fase de de ser ministro de Sanidad o que sea candidato a la Presidencia de la Generalitat de Catalunya, el señor Torra, es público y not\n",
      "\n",
      "Abascal: ros hoy hemos dejado aquí el antídoto. De usted depende, la pelota está en su tejado. Muchas gracias. .\n",
      "El balance de su gestión en dos años está siendo demolernación con la des aguantando sus insultos, dos años y cinco campañas. Usted ha cometido el error de venir aquí con un centenar de focos del virus que ha causado la muerte a 40 000 compatriotas. Ante su inacción fal su votación? Y, la tercera, ¿por qué bloquea el plan B jurídico para luchar contra la pandemia, como han hecho todos los país\n",
      "\n",
      "Sanchez: es de Europa, en Gobierno de España respeta y que serán los tribunales polacos los que decidan. En definitiva, el mitónomos conscientes de que tenemos un horizonte a largo plazo para abordar esta crisis y eso nos va a implicar 140 000 millones de euros durante los próximos seis años no solamente para dar respuesta a la crisis del COVID, sino que también se lo pido al Grupo Parlamentario Popular: con este Gobierno no hay policía patriótica  no hay parapolítica laboral del Gobierno. Le diré que no\n",
      "\n",
      "Abascal:  ser cuáles son las previsiones económicas y sociales del Gobierno? .\n",
      "Le he ofrecido una oficina nacional de víctimas del COVID, un plan de choque también en materia de legislatura. Creo que usted lo conoce bien; también los españoles. Le puedo recordar sus incumplimientos: Después de parecesario y evitar lo que a cada uno puede resultarle inaceptable. Eso es la concordia, así es como se que sentencia. No tiene ningún sentido que un político indulte a otro. Siento vergüenza y pido perdón, a mí e\n",
      "\n",
      "Sanchez: s por donde está caminando el Gobierno de España. En algunas ocasiones tratamos de equivocar los independentistas y como para renovar las instituciones en nuestro país; unidad, señoría, para renovar las instituciones en nuestro país; unidad, señoría, para renovar las instituciones en nuestro país; unidad, señoría, para renovar las instituciones en nuestro país; unidad, señoría, para renovar las instituciones en nuestro país; unidad, señoría, para renovar las instituciones en nuestro país; unidad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "modelo_abascal = tf.keras.models.load_model('./modelos/abascal.keras')\n",
    "modelo_sanchez = tf.keras.models.load_model('./modelos/sanchez.keras')\n",
    "\n",
    "modelos = [modelo_abascal, modelo_sanchez]\n",
    "politicos = [\"Abascal\", \"Sanchez\"]\n",
    "\n",
    "ultima_palabra = u\"España está en una situación\"\n",
    "turno = random.randint(0,1)\n",
    "\n",
    "for i in range(5):\n",
    "    turno = (turno + 1) % 2\n",
    "    ultima_palabra = generate_text(modelos[turno], start_string=ultima_palabra)\n",
    "    print(f\"{politicos[turno]}: {ultima_palabra}\\n\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d53d9c447d15846ca7228ba81a89f63b35afa7d922a1ed0608df97a83621769d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
